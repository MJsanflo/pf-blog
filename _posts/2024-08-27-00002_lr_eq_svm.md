---
title: '[WIP] "Logistic Regression models are equivalent to the SVMs with a smoothed max()"'
description: Discussing the claimed equivalence of two classical Machine Learning models given by a smooth approximation.
author: mjsanflo
date: 2024-08-27 12:39:00 +0800
categories: [ML, Model Equivalences]
tags: [ML, SVMs, Logistic Regression, Smooth Approximaptions]
pin: true
math: true
published: true
---

_**Disclaimer**: This posts' content has been gathered through and inspired by the atendennce to the Machine Learning 2024 course in the Informatics Faculty in the Hamburg University. Its purpose is to clarify questions with the corresponding ML reasearch group and do independent experimenting and research._

## Motivation
### Smoothing the ```max()``` in SVM's equals to Logistic regression for Classification

In one of my ML lecture exercise sessions on SVMs, [this](#proving-the-lower--and-upper-bounds-for-the-smooth-approximation)roving the lower- and upper-bounds for the smooth approximation) math proof would be concluded with the statement:
_"Logistic regression models are quivalent to SVMs with a M=1 smoothedmax(), where M is the level of smootheness"_. This while the Figure 1.0 would be shown.

![Figure 1.0](assets/img/00002/max_lf_smoothmax.png){: width="972" height="589" } _Figure 1.0 - max() in red , logistic() in green and log-sum-exp() ie smoothmax() in blue. [Desmos Interactive Graph](https://www.desmos.com/calculator/neyxo30qal)_

Apart from the similarity of the loss function graphs (```smoothedmax()``` vs ```logistic()```), given the fact that one of these model is based on statistical principles while the other is based on the geometrics of the data, this claimed equivalence was not crystal clear to me and I did not know either what the purpose of showcasing this equivalence was nor what in practice should be expected from it. This  has motivated me to dig deeper to understand the follwoing points:


1. Is this (model?) equivalence an actual [known] derived mathematical equivalence in the resulting optimization problems of each of the cases?
2. We have two practical smooth approimations involved:
    1. ```log-sum-exp()```: smoothing the ```max()``` results in the ```smoothmax()``` -  This is fully differentiable across its entire domain ie. with continuous gradient (adventageous for optimization tasks). When ```M``` approaches infinity, it approximates the ```max()```. If ```M=1```, it approximates the ```logistic()``` hinting on the initial equivalence.
    2. Smoothed ```hinge()```: the _quadratic_ smooth approximation of the ```hinge()``` within a _margin_ of ```[−M,M]```. This yields smooth segments in the non-differentiable points of the function. Outside of the ```[−M,M]``` margin, it behaves like the original ```hinge()```. The function is piece-wise differentiable, not fully smooth because the transition between the smooth segments might not be smooth.
3. Considering the two definitions above,
    1. which of them is equivalent to the ```logistic()``` function when used as a loss function?
    2. which of them when applied to SVMs is supposed to be make the model equivalent to logistic regression? I am betting it is smoothmax() (even if the performance results I got are contradictory). 
4. What can we expect performance-wise from this equivalence when appliying these models separately? 
    1. A similar loss achieved at training? A similar accuracy at prediction?
    3. Is this an equivalence only narrowed down to the loss function and not model-wise? 
    4. Which of these outperforms the other and what are the cases each could be best suited to?
5. If we have this trivial equivalence, why are we still using SVMs with ```hinge()``` loss and not with smoothed ```hinge()``` or ```smoothedmax()``` in standardized libraries?



## Background

##### Proving the lower- and upper-bounds for the smooth approximation

#TODO: Proof TBA - screenshot?

#TODO: Is this so that we know we can safely use the smoothed approximation of the max and hence use Logistic Regressions for classification problems

##### Classification exercise in SVMs exercises automatically applied with logistic regression

#TODO: Is the reason of the statment equivalent that we have directly skipped the aplication of SVMs for these classification Exercises?


## Implementation

In order to compare SVMs with ```smoothmax()``` and with the smoothly approximated ```hinge()``` with the logistic regression with ```logistic()```, 2 datasets, each of these with two classes have been generated:
1. Simple Class Imbalance (```n=15```, ```classes=2```)
2. Almost linearly Separable Data (```n=500```, ```classes=2```)

In each of the cases the problems have been solved with a manual implementation of Gradient Descent with the following parameters: ```iterations=1000```, ```M=1```, ```step_size=0.01``` (fixed). Smoothness level of ```M=1``` has been set for ```smoothmax()``` and ```hinge()```. And, a regularization parameter ```C=1``` for the logist regression.



#### Case 1: ```Smoothmax()``` ie. ```log-sum-exp()```

$$
\begin{equation}
  \frac{1}{\mathit{M}} \ln{( 1 + \exp{(\mathit{M}(1 - y \cdot f(x)))})}
\end{equation}
$$
- Fully differentiable
- For gradients to be calculated easier ie. it has continous gradients.

##### Hyperplane in Class Imbalance Example
![Figure 2.1](assets/img/00001/smoothmax_class_imbalance_hyperplane.png){: width="439" height="142" } _Hyperplane determined by SVM with ```smoothdmax()``` as a loss function, sovled by manually implemented Gradient Descent._

##### Hyperplane in Almost Linearly Separable Data Example
![Figure 2.2](assets/img/00001/smoothmax_almost_linearly_separable_hyperplane.png){: width="439" height="142" } _Hyperplane determined by SVM with ```smoothdmax()``` as a loss function, sovled by manually implemented Gradient Descent._

In the figures ```2.1``` and ```2.2``` is shown that margins have been maximized and we have a total loss of ```0.11~``` and ```0.75~``` respectively.


#### Case 2: ```Shinge()``` ie. smooth approximated ```hinge()```

$$

\ell_{\text{smooth}}(x) =
\begin{cases} 
0 & \text{if } 1 - y \cdot f(x) < \mathit{-M}, \\
\frac{(\mathit{M}-(1 - y \cdot f(x)))^{2}}{4 \mathit{M}}  & \text{if } \mathit{-M} \leq 1 - y \cdot f(x) \leq  \mathit{M}, \\
1 - y \cdot f(x) & \text{if } 1 - y \cdot f(x) > \mathit{M},
\end{cases}

$$

#TODO: Add characteristics of the function

##### Hyperplane in Class Imbalance Example
![Figure 2.3](assets/img/00001/shinge_class_imbalance_hyperplane.png){: width="439" height="142" } _Hyperplane determined by SVM with the smoothed ```hinge()``` as a loss function, sovled by manually implemented Gradient Descent._

##### Hyperplane in Almost Linearly Separable Data Example
![Figure 2.4](assets/img/00001/shinge_almost_linearly_separable_hyperplane.png){: width="439" height="142" } _Hyperplane determined by SVM with the smoothed ```hinge()``` as a loss function, sovled by manually implemented Gradient Descent._

#TODO: Add/Update figure conclusions


#### Case 3: Logistic Regression with ```logistic()``` ie Entrophy function(?)

$$
\begin{equation}
  \ln{1 + \exp{-x}}
\end{equation}
$$

#TODO: Add characteristics of the function

##### Hyperplane in Class Imbalance Example
![Figure 2.5](assets/img/00001/logistic_class_imbalance_hyperplane.png){: width="439" height="142" } _Hyperplane determined by logistic regression with ```logistic()``` as a loss function, sovled by manually implemented Gradient Descent._

##### Hyperplane in Almost Linearly Separable Data Example
![Figure 2.6](assets/img/00001/logistic_almost_linearly_separable_hyperplane.png){: width="439" height="142" } _Hyperplane determined by logistic regression with ```logistic()``` as a loss function, sovled by manually implemented Gradient Descent._

#TODO: Add/Update figure conclusions

[Google Colab with Source Code](https://colab.research.google.com/drive/1Dgjhq1lk6CxCZKBjVNKpvStc7j9UvLcm)


## Discussion
#TODO: TBA


## A little bit of history: Logistic Function vs Hinge Loss
Very similar, one very studied and hardly backed up with math.
The one can be utilized with Newtons method and would be the fastest implementation.
The other is not known to be as fast.


#### Resources
- [Why isn’t the logistic regression a Max-Margin classifier?](https://medium.com/@anuragbms/why-isnt-the-logistic-regression-a-max-margin-classifier-4c6961cf23b1)
- [LR vs SVM applications, bachelor work](https://kurser.math.su.se/pluginfile.php/20130/mod_folder/content/0/Kandidat/2018/2018_20_report.pdf%3Fforcedownload%3D1)


#### Potential inspiration sources for further investigation:
- [LR vs SVM performance](https://www.quora.com/When-does-the-logistic-regression-perform-better-than-a-linear-SVM)
- [LR vs SVM on Echocardiogram Dataset](https://iptek.its.ac.id/index.php/inferensi/article/view/14121)
- [Comparing SVM and logistic regression](https://stats.stackexchange.com/questions/95340/comparing-svm-and-logistic-regression)
- [Highly smooth minimization of non-smooth problems](https://proceedings.mlr.press/v125/bullins20a/bullins20a.pdf)

