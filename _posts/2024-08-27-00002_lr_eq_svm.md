---
title: '[WIP] "Logistic Regression models are equivalent to the SVMs with a smoothed max()"'
description: Discussing the claimed equivalence of two classical Machine Learning models enabled by a smoot approximation.
author: mjsanflo
date: 2024-08-27 12:39:00 +0800
categories: [ML, Model Equivalences]
tags: [ML, SVMs, Logistic Regression, Smooth Approximaptions]
pin: true
math: true
---

_**Disclaimer**: The following content has been gathered through atendennce to the Machine Learning 2024 course in the Informatics Faculty in the Hamburg University and has been layed down in order to clarify questions with the corresponding ML reasearch group._

## Abstract

One of my ML lecture exercise session on SVMs would be concluded with the statement:
_"This way we can see that logistic regression models are quivalent to SVMs with a M=1 smoothed max(), where M is the level of smootheness"_. This while the Figure 1.0 would be shown.

![Figure 1.0](assets/img/00002/max_lf_smoothmax.png){: width="972" height="589" } _Figure 1.0 - max() in red , logistic() in green and log-sum-exp() ie smoothedmax() in blue._

Besides the shape similarity of the loss functions shown in the graph this model equivalence was not trivial to me and has motivated me to dig a little deeper to understand the follwoing points:

#TODO: TDR

1. Is this an actual derived mathematical equivalence in the Optimization Problems fromulated for each of the cases?
2. Considering the following
    1. smoothing the ```max()``` results in the ```smoothmax()``` ie. ```log-sum-exp()``` which is fully differentiable across its entire domain ie. with continuous gradient (which is adventageous for optimization tasks).  And, when ```M``` approaches infinity then it basically becomes the ```max()``` hence we need ```M=1``` for the equivalence.
    2. ```shinge()``` is the quadratic smooth approximation of the ```hinge()``` within a margin of ```[−M,M]```. This yields smooth segments in the non-differentiable points and outside of that ```[−M,M]``` margin it behaves like the original ```hinge()```. The function is piecewise differentiablenot and fully smooth across all input values (because the transition between the smooth segments might not be smooth).
    1. TODO: we might still want to visualize ```smoothmax()``` and ```shinge()``` to compare them
3. with ```2.1``` and ```2.2``` in mind, which of them is equivalent to  ```logistic()``` in logistic regression? I am betting it it to smoothmax() (even if the performance results I got are contradictory). 
    1. Which of them do we need to have in the SVM for it to be comparable to logistic regression with ```logistic()``` as loss?
4. What can we expect model-performance-wise from this equivalence? 
    1. A similar loss achieved at training? 
    2. A similar accuracy at prediction?
    3. Is this an equivalence only narrowed down to the loss function and not model-wise? Given the fact that one of these model is pure statistical while the other is bades on the geometrics of the data.
    4. Which of these outperforms the other and what are the cases they could be best suited to.
5. If we have this trivial equivalence, why are we still using SVMs with Hinge Loss and not Shinge in standardized libraries?



## Background

##### Proving that the smoothmax()'s lower- and upper-bounds.
Is this so that we know we can safely use the smoothed approximation of the max and hence use Logistic Regressions for classification problems

##### Classification exercise in SVMs exercises automatically applied with logistic regression

Is the reason of the statment equivalent that we have directly skipped the aplication of SVMs for these classification Exercises


## Implementation

In order to compare SVMs with ```smoothmax()``` and with the smoothly approximated ```hinge()``` with the logistic regression with ```logistic()```, 2 datasets, each of these with two classes have been generated:
1. Simple Class Imbalance (```n=15```, ```classes=2```)
2. Almost linearly Separable Data (```n=500```, ```classes=2```)

In each of the cases the problems have been solved with a manual implementation of Gradient Descent with the following parameters: ```iterations=1000```, ```M=1```, ```step_size=0.01``` (fixed). Smoothness level of ```M=1``` has been set for ```smoothmax()``` and ```hinge()```. And, a regularization parameter ```C=1``` for the logist regression.



#### Case 1: ```Smoothmax()``` ie. ```log-sum-exp()```

$$
\begin{equation}
  \frac{1}{\mathit{M}} \ln{( 1 + \exp{(\mathit{M}(1 - y \cdot f(x)))})}
\end{equation}
$$
- Fully differentiable
- For gradients to be calculated easier ie. it has continous gradients.

##### Hyperplane in Class Imbalance Example
![Figure 2.1](assets/img/00001/smoothmax_class_imbalance_hyperplane.png){: width="439" height="142" } _Hyperplane determined by SVM with ```smoothdmax()``` as a loss function, sovled by manually implemented Gradient Descent._

##### Hyperplane in Almost Linearly Separable Data Example
![Figure 2.2](assets/img/00001/smoothmax_almost_linearly_separable_hyperplane.png){: width="439" height="142" } _Hyperplane determined by SVM with ```smoothdmax()``` as a loss function, sovled by manually implemented Gradient Descent._

In the figures ```2.1``` and ```2.2``` is shown that margins have been maximized and we have a total loss of ```0.11~``` and ```0.75~``` respectively.


#### Case 2: ```Shinge()``` ie. smooth approximated ```hinge()```

$$

\ell_{\text{smooth}}(x) =
\begin{cases} 
0 & \text{if } 1 - y \cdot f(x) < \mathit{-M}, \\
\frac{(\mathit{M}-(1 - y \cdot f(x)))^{2}}{4 \mathit{M}}  & \text{if } \mathit{-M} \leq 1 - y \cdot f(x) \leq  \mathit{M}, \\
1 - y \cdot f(x) & \text{if } 1 - y \cdot f(x) > \mathit{M},
\end{cases}

$$

#TODO: Add characteristics of the function

##### Hyperplane in Class Imbalance Example
![Figure 2.3](assets/img/00001/shinge_class_imbalance_hyperplane.png){: width="439" height="142" } _Hyperplane determined by SVM with the smoothed ```hinge()``` as a loss function, sovled by manually implemented Gradient Descent._

##### Hyperplane in Almost Linearly Separable Data Example
![Figure 2.4](assets/img/00001/shinge_almost_linearly_separable_hyperplane.png){: width="439" height="142" } _Hyperplane determined by SVM with the smoothed ```hinge()``` as a loss function, sovled by manually implemented Gradient Descent._

#TODO: Add/Update figure conclusions


#### Case 3: Logistic Regression with ```logistic()``` ie Entrophy function(?)

$$
\begin{equation}
  \ln{1 + \exp{-x}}
\end{equation}
$$

#TODO: Add characteristics of the function

##### Hyperplane in Class Imbalance Example
![Figure 2.5](assets/img/00001/logistic_class_imbalance_hyperplane.png){: width="439" height="142" } _Hyperplane determined by logistic regression with ```logistic()``` as a loss function, sovled by manually implemented Gradient Descent._

##### Hyperplane in Almost Linearly Separable Data Example
![Figure 2.6](assets/img/00001/logistic_almost_linearly_separable_hyperplane.png){: width="439" height="142" } _Hyperplane determined by logistic regression with ```logistic()``` as a loss function, sovled by manually implemented Gradient Descent._

#TODO: Add/Update figure conclusions

[Google Colab with Source Code](https://colab.research.google.com/drive/1Dgjhq1lk6CxCZKBjVNKpvStc7j9UvLcm)


## Discussion
#TODO: TBA


## A little bit of history: Logistic Function vs Hinge Loss
Very similar, one very studied and hardly backed up with math.
The one can be utilized with Newtons method and would be the fastest implementation.
The other is not known to be as fast.


#### Resources
- [Why isn’t the logistic regression a Max-Margin classifier?](https://medium.com/@anuragbms/why-isnt-the-logistic-regression-a-max-margin-classifier-4c6961cf23b1)
- [LR vs SVM applications, bachelor work](https://kurser.math.su.se/pluginfile.php/20130/mod_folder/content/0/Kandidat/2018/2018_20_report.pdf%3Fforcedownload%3D1)


#### Potential inspiration sources for further investigation:
- [LR vs SVM performance](https://www.quora.com/When-does-the-logistic-regression-perform-better-than-a-linear-SVM)
- [LR vs SVM on Echocardiogram Dataset](https://iptek.its.ac.id/index.php/inferensi/article/view/14121)
- [Comparing SVM and logistic regression](https://stats.stackexchange.com/questions/95340/comparing-svm-and-logistic-regression)
- [Highly smooth minimization of non-smooth problems](http://proceedings.mlr.press/v125/bullins20a/bullins20a.pdf)

