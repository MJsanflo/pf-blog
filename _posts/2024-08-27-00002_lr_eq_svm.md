---
title: '[WIP] "Logistic Regression models are equivalent to the SVMs with a smoothed max()"'
description: Discussing the claimed equivalence of two classical Machine Learning models enabled by a smoot approximation.
author: mjsanflo
date: 2024-08-27 12:39:00 +0800
categories: [ML, Model Equivalences]
tags: [ML, SVMs, Logistic Regression, Smooth Approximaptions]
pin: true
math: true
---

_**Disclaimer**: The following content has been gathered through atendennce to the Machine Learning 2024 course in the Informatics Faculty in the Hamburg University and has been layed down in order to clarify questions with the corresponding ML reasearch group._

## Abstract

One of my ML lecture exercise session on SVMs would be concluded with the statement:
_"This way we can see that logistic regression models are quivalent to SVMs with a M=1 smoothed max(), where M is the level of smootheness"_. This while the Figure 1.0 would be shown.

![Figure 1.0](assets/img/00002/max_lf_smoothmax.png){: width="972" height="589" } _Figure 1.0 - max() in red , logistic() in green and log-sum-exp() ie smoothedmax() in blue._

Besides the shape similarity of the loss functions shown in the graph this model equivalence was not trivial to me and has motivated me to dig a little deeper to understand the follwoing points:

1. Is this an actual derived mathematical equivalence in the Optimization Problems fromulated for each of the cases?
2. Considering the following
  1. smoothing the ```max()``` results in the ```smoothmax()``` ie. ```log-sum-exp()``` which is fully differentiable across its entire domain ie. with continuous gradient (which is adventageous for optimization tasks).  And, when ```M``` approaches infinity then it basically becomes the ```max()``` hence we need ```M=1``` for the equivalence.
  2. ```shinge()``` is the quadratic smooth approximation of the ```hinge()``` within a margin of ```[−M,M]```. This yields smooth segments in the non-differentiable points and outside of that ```[−M,M]``` margin it behaves like the original ```hinge()```. The function is piecewise differentiablenot and fully smooth across all input values (because the transition between the smooth segments might not be smooth).
    1. TODO: we might still want to visualize ```smoothmax()``` and ```shinge()``` to compare them
3. with ```2.1``` and ```2.2``` in mind, which of them is equivalent to  ```logistic()``` in logistic regression? I am betting it it to smoothmax() (even if the performance results I got are contradictory). 
  1. Which of them do we need to have in the SVM for it to be comparable to logistic regression with ```logistic()``` as loss?
4. What can we expect model-performance-wise from this equivalence? 
  1. A similar loss achieved at training? 
  2. A similar accuracy at prediction?
  3. Is this an equivalence only narrowed down to the loss function and not model-wise? Given the fact that one of these model is pure statistical while the other is bades on the geometrics of the data.
  4. Which of these outperforms the other and what are the cases they could be best suited to.
5. If we have this trivial equivalence, why are we still using SVMs with Hinge Loss and not Shinge in standardized libraries?



## Background

### Proving that the smoothmax()'s lower- and upper-bounds.
Is this so that we know we can safely use the smoothed approximation of the max and hence use Logistic Regressions for classification problems

### Classification exercise in SVMs exercises automatically applied with logistic regression

Is the reason of the statment equivalent that we have directly skipped the aplication of SVMs for these classification Exercises


## Implementation

#### ```Smoothmax()``` ie. ```log-sum-exp()```

$$
\begin{equation}
  \sum_{n=1}^\infty 1/n^2 = \frac{\pi^2}{6}
\end{equation}
$$

Class imbalance example and linearly separable data examples

#### ```Shinge()``` ie. smooth approximated ```hinge()```

$$
\begin{equation}
  \sum_{n=1}^\infty 1/n^2 = \frac{\pi^2}{6}
\end{equation}
$$

Class imbalance example and linearly separable data examples


#### Logistic Regression with ```logistic()``` ie Entrophy function(?)

$$
\begin{equation}
  \sum_{n=1}^\infty 1/n^2 = \frac{\pi^2}{6}
\end{equation}
$$

Class imbalance example and linearly separable data examples




## Discussion





## A little bit of history: Logistic Function vs Hinge Loss
Very similar, one very studied and hardly backed up with math.
The one can be utilized with Newtons method and would be the fastest implementation.
The other is not known to be as fast.


Potential inspiration sources for further investigation:
https://www.quora.com/When-does-the-logistic-regression-perform-better-than-a-linear-SVM
https://kurser.math.su.se/pluginfile.php/20130/mod_folder/content/0/Kandidat/2018/2018_20_report.pdf%3Fforcedownload%3D1
https://iptek.its.ac.id/index.php/inferensi/article/view/14121

